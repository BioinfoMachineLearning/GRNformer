# lightning.pytorch==2.2.1
seed_everything: 123

data:
  class_path: src.datamodules.grn_datamodule.GRNDataModule
  init_args:
    dataset_dir: 'test'
    batch_size: 8
    num_workers: 8

model:
  class_path: src.models.grnformer.model.GRNFormerLitModule
  init_args:
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 0.001
        weight_decay: 0.0

    scheduler:
      class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
      init_args:
        mode: min
        factor: 0.1
        patience: 5
        min_lr: 0.00001
        eps: 1e-10

    # model:
    #   class_path: src.models.CryoTEN.network.CryoTEN
    #   init_args:
    #     in_channels: 1
    #     out_channels: 1
    #     img_size: 48
    #     feature_size: 16
    #     hidden_size: 256
    #     num_heads: 4
    #     depths: [3, 3, 3, 3]
    #     dims: [32, 64, 128, 256]
    #     pos_embed: "perceptron"
    #     norm_name: "instance"
    #     dropout_rate: 0.0
    #     do_ds: False

    # embed:
    #   class_path: src.models.CryoTEN.network.CryoTEN
    #   init_args:
    #     in_channels: 1

    # edgepred:
    #   class_path: src.models.CryoTEN.network.CryoTEN
    #   init_args:
    #     in_channels: 1

    # edgepred:
    #   class_path: src.models.CryoTEN.network.CryoTEN

trainer:
  accelerator: gpu
  strategy: ddp
  devices: -1
  num_nodes: 1
  max_epochs: 500
  min_epochs: 1
  log_every_n_steps: 1
  deterministic: true
  detect_anomaly: true
  gradient_clip_val: 0.5

  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        monitor: 'valid_loss'
        dirpath: null # directory to save the model file
        save_top_k: 10
        filename: 'GRNFormer_beeline_{epoch:02d}_{valid_loss:6f}'

    # - class_path: lightning.pytorch.callbacks.RichModelSummary
    #   init_args:
    #     max_depth: -1

    - class_path: lightning.pytorch.callbacks.RichProgressBar
      init_args:
        leave: True

    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: 'epoch'
    
    # - class_path: lightning.pytorch.callbacks.EarlyStopping
    #   init_args:
    #     monitor: 'valid_loss'
    #     mode: 'min'
    #     min_delta: 0.0
    #     patience: 20

  logger:
    - class_path: lightning.pytorch.loggers.WandbLogger
      init_args:
        id: null # pass correct id to resume experiment!
        name: "retrained_lrelu_model" # name of the run (normally generated by wandb). use --trainer.logger.name to specify it.
        save_dir: "logs/retrained_lrelu_model" # directory to save the wandb logs
        project: "GRNFormer"
        entity: "aghktb" # set to name of your wandb team

ckpt_path: null
